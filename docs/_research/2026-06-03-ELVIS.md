---
title: "ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark"
alt_title: Low-Light Video Instance Segmentation
categories: [Low-Light]
about: We propose a framework for improving the performance of state-of-the-art Video Instance Segmentation (VIS) methods on low-light videos.
authors:
  names: [J. Lin, R. Lin, Y. Li, D. Bull, N. Anantrasirichai]
  institute: [University of Bristol, University of Bristol, University of Bristol, University of Bristol, University of Bristol, University of Bristol]
  urls: [https://joannelin168.github.io/, https://lrr-rachel.github.io/, https://research-information.bris.ac.uk/en/persons/yini-li/, https://david-bull.github.io/, https://pui-nantheera.github.io/]
funders: MyWorld Strength in Places
collaborators: 
thumbnail: assets/images/publication_thumbnails/ELVIS.jpg
poster:
poster_thumbnail:
video: assets/videos/ELVIS_Bristol_demo.mp4
publication:
    paper:
    arxiv: http://arxiv.org/abs/2512.01495
    journal: IEEE/CVF Conference on Computer Vision and Pattern Recognition
    journal_short: CVPR
    year: 2026
    journal_display: CVPR 2026
    supplementary:
github: https://github.com/JoanneLin168/ELVIS
citation: |
    @article{lin2026elvis,
        author={Lin, Joanne and Lin, Ruirui and Li, Yini and Bull, David and Anantrasirichai, Nantheera},
        title={ELVIS: Enhance Low-Light for Video Instance Segmentation in the Dark},
        year={2026},
        publisher={CVPR}
    }
featured: true

layout: post
---

> NOTE: This page will be updated soon with the camera-ready version!

Video instance segmentation (VIS) for low-light content remains highly challenging for both humans and machines alike, due to adverse imaging conditions including noise, blur and low-contrast. The lack of large-scale annotated datasets and the limitations of current synthetic pipelines, particularly in modeling temporal degradations, further hinder progress. Moreover, existing VIS methods are not robust to the degradations found in low-light videos and, as a result, perform poorly even when finetuned on low-light data. In this paper, we introduce <b>ELVIS</b> (<b>E</b>nhance <b>L</b>ow-light for <b>V</b>ideo <b>I</b>nstance <b>S</b>egmentation), a novel framework that enables effective domain adaptation of state-of-the-art VIS models to low-light scenarios. ELVIS comprises an unsupervised synthetic low-light video pipeline that models both spatial and temporal degradations, a calibration-free degradation profile synthesis network (VDP-Net) and an enhancement decoder head that disentangles degradations from content features. ELVIS improves performances by up to <b>+3.7AP</b> on the synthetic low-light YouTube-VIS 2019 dataset.
